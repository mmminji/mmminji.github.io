---
layout: post
title:  "선형대수와 통계학으로 배우는 머신러닝 with 파이썬 8장"
date:   2021-06-14
categories: kNN LinearRegression LogisticRegression NaiveBayes DecisionTree SupportVectorMachine
---
[Book Review] 한줄정리

# 8 지도 학습

## 8.1 지도 학습 개요
- 지도 학습(Supervised learning) : 타겟 데이터(정답)가 표시된 데이터를 학습시키는 것
    - 분류(Classification) : 타겟이 범주형(클래스)인 경우      e.g. 사과, 배
    - 회귀(Regression) : 타겟이 연속형 숫자인 경우            e.g. 중학생의 키
- 비지도 학습(Unsupervised learning) : 타겟 데이터를 모르는 데이터를 학습시키는 것

## 8.3 k-최근접 이웃 알고리즘 [>코드<](https://github.com/mmminji/Machine-Learning/blob/master/8.3.kNN.py)
k-Nearest Neighbor(kNN, k-최근접 이웃)이란, 비교 대상이 되는 데이터 주변에 가장 가까이 존재하는 k개의 데이터와 비교하여 예측하는 것이다.  
분류에서는 테스트 데이터의 주변 반경에서 가장 가까운 k개의 데이터 중 가장 많이 존재하는 클래스로 판별한다. 회귀에서는 주변 k개의 데이터 평균값으로 예측한다.  
단, kNN은 게으른 학습 방법을 사용한다. 게으른 학습 방법이란, 학습 데이터 전체를 메모리상에 보관하면서 테스트 데이터가 새로 들어왔을 때 바로 학습하는 것이다. 이는 추가적인 학습 시간 없이 바로 예측 가능하다는 장점이 있지만 학습 데이터를 항상 저장하고 있어 메모리 문제로 사용 불가능 할 수도 있다는 단점이 있다. 반대로 열정적 학습은 일정 기간 학습 데이터를 학습시킨 후, 그 모형을 적용하여 테스트 데이터를 예측하는 것이다. 메모리 측면에서는 효율적이지만 학습 시간이 오래 걸린다.

## 8.4 선형 회귀 분석 [>코드<](https://github.com/mmminji/Machine-Learning/blob/master/8.4.LinearRegression.py)
Linear Regression(선형 회귀 분석)이란, X변수와 y변수의 선형 관계를 파악하는 알고리즘이다. 이 때, 가중치(w)는 학습 데이터로부터 최소 제곱법을 사용하여 구한다. 
- 최소 제곱법 : 오차(실제-예측)의 제곱합이 최소가 되는 추정량을 구하는 방법  
- Ridge Regression(릿지 회귀 분석)이란, 추정하려는 가중치가 커지는 것을 방지하기 위해 L2 제약식을 사용한 회귀 분석 방법이다. 릿지 회귀 분석은 편향이 존재한다는 특징이 있어 릿지 추정량의 기댓값은 실제 파라미터와 동일하지 않다.  
- Lasso Regression(라쏘 회귀 분석)이란, 회귀 분석에 L1 제약식을 적용한 방법이다. 라쏘 추정량에서는 가중치 값이 0이 될 수 있기 때문에 변수 선택으로 사용할 수 있다.  
- Elastic net(엘라스틱 넷)이란, 릿지 회귀 분석과 라쏘 회귀 분석을 합쳐 놓은 형태로 그 비율 또한 조절할 수 있다.

## 8.5 로지스틱 회귀 분석 [>코드<](https://github.com/mmminji/Machine-Learning/blob/master/8.5.LogisticRegression.py)
Logistic Regression(로지스틱 회귀 분석)이란, 분류를 위해 클래스의 확률 값을 예측하는 방법이다. 선형 회귀 식에 시그모이드 함수를 적용하여 0부터 1사이의 값이 나오게 한다. 이 때, 실패 확률 대비 성공 확률의 비를 오즈비(odds ratio)라고 하며, 오즈비에 로그를 취한 값은 로짓(logit)이다. 

## 8.6 나이브 베이즈 [>코드<](https://github.com/mmminji/Machine-Learning/blob/master/8.6.NaiveBayes.py)
Naive bayes(나이브 베이즈)는 서로 조건부 독립인 변수를 가정하고, 베이즈 이론을 기반으로 하는 알고리즘이다. 조건부 확률에서의 조건을 이용하여 각각의 클래스 확률을 추정하는 것이다. 
- 베이즈 정리 : 두 확률 변수의 사전 확률과 사후 확률 사이의 관계를 나타낸 정리  
![](https://github.com/mmminji/mmminji.github.io/blob/main/assets/post_pics/8.6-Bayes.PNG?raw=true){: width="300" height="80"}  
![](https://github.com/mmminji/mmminji.github.io/blob/main/assets/post_pics/8.6-Naive_bayes.PNG?raw=true){: width="500" height="300"}

## 8.7 의사 결정 나무 [>코드<](https://github.com/mmminji/Machine-Learning/blob/master/8.7.DecisionTree.py)
Decision Tree(의사 결정 나무)는 나무 구조에서 가지를 치듯 여러 조건을 조합하여 예측하는 알고리즘이다. 의사 결정 나무에서 다음의 용어들이 사용된다.  
![](https://github.com/mmminji/mmminji.github.io/blob/main/assets/post_pics/8.7-tree.PNG?raw=true){: width="800" height="500"}  
- 엔트로피 : 여러 조건 후보 중 첫 번째 조건을 선정하는 기준은 엔트로피를 사용한 성능 평가이다. 엔트로피는 불순도 정도를 측정하고 낮을수록 좋다.
- 불순도 : 서로 다른 데이터가 얼마나 섞여 있는지 의미한다. 불순도가 낮을수록 데이터가 섞여 있지 않다는 것이다.
![](https://github.com/mmminji/mmminji.github.io/blob/main/assets/post_pics/8.7-entropy.PNG?raw=true){: width="300" height="60"}
- 지니계수 : 불순도를 측정하는 또 다른 방법으로, 데이터 셋에서 랜덤으로 선택한 데이터에 임의로 라벨링을 정했을 때 틀릴 확률을 의미한다.  
- 회귀 나무 : 타겟 데이터가 연속형인 경우는 각 노드의 평균값으로 예측하게 된다.  
--> 의사 결정 나무는 모형을 해석하기 쉽다는 장점이 있는 반면, 오버피팅된다는 단점이 있다. 이를 보완한 방법은 랜덤 포레스트이다.

## 8.8 서포트 벡터 머신 [>코드<](https://github.com/mmminji/Machine-Learning/blob/master/8.8.SupportVectorMachine.py)
Support Vector Machine(SVm, 서포트 벡터 머신)이란, 서포트 벡터 간 너비(=마진)를 최대화하여 결정 경계를 정의하는 알고리즘이다. 이 때, 서포트 벡터는 클래스를 구분하는 경계선에 위치하는 데이터를 의미한다.  
![](https://github.com/mmminji/mmminji.github.io/blob/main/assets/post_pics/8.8-svm.PNG?raw=true){: width="700" height="500"}
- 소프트 마진 : 기존 서포트 벡터 머신의 기준을 완화해 잘못 분류된 데이터를 어느 정도 허용하는 방법이다.
- 커널 서포트 벡터 머신 : 피처 공간을 변경하여 서포트 벡터 머신을 적용한 것이다.
- 서포트 벡터 회귀 : 분류에서 클래스를 나누는 결정 경계가 회귀에서는 회귀 직선이 된다.

## 8.9 크로스 밸리데이션 실습 [>코드<](https://github.com/mmminji/Machine-Learning/blob/master/8.9.CrossValidation.py)


이 내용은 **선형대수와 통계학으로 배우는 머신러닝 with 파이썬 : 최적화 개념부터 텐서플로를 활용한 딥러닝까지** 책을 기반으로 공부한 자료입니다.
