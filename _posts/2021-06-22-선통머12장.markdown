---
layout: post
title:  "딥러닝"
date:   2021-06-22
categories: ANN Perceptron BackPropagation ActivationFunction BatchNormalization Dropout
---
[Book Review] 한줄정리

# 12 딥러닝

## 12.1 딥러닝
Artificial Neural Network(인공 신경망)이란, 임계치를 넘긴 자극을 신호로 전달받고 그에 해당하는 반응을 하는 생명 과학 분야의 아이디어를 머신러닝에 도입한 것을 말한다. 이러한 신경망을 기반으로 학습하는 것이 Deep Learning(딥러닝)이다. 초기 딥러닝은 XOR 문제를 해결하기 위해 등장했으며 점차 복잡한 문제를 푸는데 다양하게 사용되고 있다.

## 12.2 퍼셉트론 [>코드<](https://github.com/mmminji/ML-DL-STUDY/blob/master/선형대수와%20통계학으로%20배우는%20머신러닝%20with%20파이썬/12.2.Perceptron.py)
Perceptron(퍼셉트론)은 신경망의 최소 단위로 입력값(+편향)의 가중합이 활성화 함수를 거치면서 출력값으로 반환된다.

## 12.3 인공 신경망으로 하는 딥러닝

### 12.3.1 인공 신경망
Multi-layer perceptron(다층 퍼셉트론)은 퍼셉트론의 층을 여러 개 쌓아 기존의 데이터 공간을 변형시키는 것이다. 이렇게 다수의 뉴런을 사용해 만든 것이 인공 신경망이며, 입력층과 출력층 사이의 다수의 은닉층이 존재한다. 분류 태스크에서 입력층의 노드 개수는 변수(feature) 개수와 동일하며 출력층은 각 클래스의 스코어를 나타내므로 분류 클래스 개수와 같다. 

### 12.3.2 오차 역전파
Back Propagation(오차 역전파)는 출력층에서 입력층 방향으로 반대로 거슬러 올라가며 오차를 기반으로 가중치를 수정하여 성능을 개선시키는 방법이다. 신경망은 이러한 오차 역전파 방법을 사용하여 학습된다.  
 (1) 가중치 초기화  
 (2) 순전파를 통한 출력값 계산  
 (3) 비용함수(e.g. 오차 제곱합) 정의 및 활성화 함수의 1차 미분식 구하기  
 (4) 역전파를 통한 1차 미분값 계산  
 (5) 파라미터(가중치와 편향값) 업데이트  
 (2)~(5) 반복

### 12.3.3 활성화 함수
Activation function(활성화 함수)은 가중합 결과를 활성화할지 말지를 결정하는 함수로, 은닉층과 출력층의 활성화 함수를 다르게 적용할 수 있다.
- 계단 함수(Step) : 임계값 이하일 경우는 0, 초과하면 1을 출력 --> 사용하기 간단하지만 미분이 불가능
- 부호 함수(Sign) : 양수면 1, 0이면 0, 음수면 -1을 출력
- 시그모이드 함수(Sigmoid) : 0과 1사이의 값을 출력 --> 미분 반복시 그래디언트 소실 문제를 발생시켜 학습 속도가 느려짐
- 하이퍼볼릭 탄젠트 함수(Hyperbolic tangent, tanh) : 시그모이드 함수를 변형시켜 -1부터 1사이의 값을 출력
- 렐루 함수(ReLU) : 입력값과 0의 최대값을 출력 --> 상한선이 없음
- 리키 렐루(Leaky ReLU) : 렐루 함수에서 0 이하일 경우 알파값을 곱해 출력 --> 0으로 일정하지 않게함
- 항등 함수(Identity) : 입력을 그대로 출력 --> 범위 제한이 없으며 미분하면 항상 일정
- 소프트맥스 함수(Softmax) : 주로 분류 문제의 출력층에 사용하는 함수로 0과 1사이의 값을 출력 --> 확률에 대응해서 해석 가능

### 12.3.4 배치 정규화
Batch Normalization(배치 정규화)은 해당층 값의 분포를 변경하는 방법으로, 평균과 분산을 고정시키는 방법이다. 이는 그래디언트 소실 문제를 줄여 신경망의 학습 속도를 향상시킬 수 있다. 

### 12.3.5 드롭아웃
Dropout(드롭아웃)은 신경망의 모든 노드를 사용하지 않고 일부 노드만 사용하는 방법이다. 이는 노드 수가 줄어들어 연산량을 줄일 수 있고 오버피팅을 방지할 수 있다.

### 배치사이즈(batch size), 에포크(epoch), 반복(iteration)의 차이
- 배치사이즈 : 하나의 미니 배치에 속하는 데이터 개수
- 에포크 : 전체 학습 데이터셋이 신경망을 통과한 횟수
- 반복 : 1에포크에 필요한 미니 배치 수 --> 가중치 파라미터는 미니 배치당 한 번씩 업데이트

### 12.3.7 분류 신경망 [>코드<](https://github.com/mmminji/ML-DL-STUDY/blob/master/선형대수와%20통계학으로%20배우는%20머신러닝%20with%20파이썬/12.3.7.ClassificationANN.py)
### 12.3.8 회귀 신경망 [>코드<](https://github.com/mmminji/ML-DL-STUDY/blob/master/선형대수와%20통계학으로%20배우는%20머신러닝%20with%20파이썬/12.3.8.RegressionANN.py)

이 내용은 **선형대수와 통계학으로 배우는 머신러닝 with 파이썬 : 최적화 개념부터 텐서플로를 활용한 딥러닝까지** 책을 기반으로 공부한 자료입니다.
